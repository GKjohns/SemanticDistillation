# Semantic Distillation Pipeline — Improvement Plan

## Executive Summary

After analyzing 5 multi-iteration runs (3 legacy mode, 2 validation-split mode) across `iteration_logs/` and `demo_run/`, I've identified **one critical bug** that invalidates most evaluation metrics, several **structural issues** with how the pipeline learns and evaluates, and a set of **modeling improvements** that should meaningfully improve both self-assessment and self-improvement capabilities.

---

## Part 1: Bugs & Broken Mechanics (Fix First) ✅ **COMPLETED**

### 1.1 CRITICAL: Feature Cache Ignores Feature Config ✅ **COMPLETED**

**The problem:** `FeatureCache` in `src/utils.py` keys cached features solely by `md5(text)`. When the feature config changes between iterations (which is the whole point), cached features from the OLD config are served for any text already seen.

**Impact:** The validation set is extracted once in iteration 1, then **never re-extracted** in subsequent iterations. The model trains on genuinely new features (from a fresh train fold) but evaluates against stale iteration-1 features. This explains:
- Per-class metrics are **identical** across ALL iterations in both validation-split runs (frozen at iteration-1 values)
- Catastrophic validation accuracy drops (0.83 → 0.17) when the model learns column names that don't exist in the stale validation feature matrix
- The entire self-improvement loop is broken — the pipeline can't actually measure whether new features help

**Fix:** Include a hash of the feature config in the cache key:
```python
def _cache_key(self, text: str, feature_config_hash: str = "") -> str:
    return hashlib.md5(f"{feature_config_hash}:{text}".encode()).hexdigest()
```
The `SemanticDistiller` should compute a stable hash of its feature config and pass it through to all cache operations.

### 1.2 Per-Class Metrics Computed on Wrong Model ✅ **COMPLETED**

**The problem:** In `_run_iteration_with_validation` (line 1154), per-class metrics are generated by `get_predictions_with_details()`, which fits a **new, separate model** on the validation data and predicts on that same data. These are not the per-class metrics of the actual train→validate pipeline.

**Fix:** Compute per-class metrics directly from the `fitted_model` predictions on the validation set, which are already available from `fit_and_evaluate_split()`. Return `val_preds` from that function and compute metrics from those predictions.

### 1.3 Column Mismatch Between Train and Validation Feature Matrices ✅ **COMPLETED**

**The problem:** When features change between iterations but validation features are cached, the `build_feature_matrix` call for train vs validation can produce different columns. The current code tries to handle this (lines 784-787) but the fundamental issue is that stale cached features produce a completely wrong feature matrix.

**Fix:** This is resolved by fixing the cache (1.1). But additionally, add an assertion that train and validation feature configs match at evaluation time.

---

## Part 2: Structural Issues with the Learning Loop

### 2.1 Sample Analysis Overfits to ~10 Examples ✅ **COMPLETED**

**Current state:** `analyze_samples()` shows the LLM 5 correct + 5 incorrect samples. The LLM constructs elaborate theories from these 10 examples and proposes hyper-specific features (e.g., a 9-category `canonical_trope_cluster`).

**Evidence from runs:**
- Run 1, Iter 1 sample analysis proposes "AITA trope tagger" with categories like `seat_swap_request`, `roommate_food_theft_or_deterrent` — clearly overfitting to the specific examples seen
- Features like `mutual_access_precedent_context` (5 categories) are proposed based on a single phone-snooping example

**Proposed fix — Two-stage analysis:**

1. **Broad pattern LLM** (new): Before the iteration loop, have an LLM review a larger sample (100-200 posts, just text + verdict, no features) and summarize: "What general patterns distinguish NTA from YTA posts?" This produces domain-level priors that are stable across iterations.

2. **Focused error analysis** (existing, refined): The per-iteration analysis still looks at misclassified samples, but is now grounded by the broad patterns. Its prompt should reference the broad analysis and ask "given these known patterns, what specific signals is the current feature set missing?" rather than building theories from scratch.

The broad analysis is run once per session (cheap — just text, no extraction) and provides a stable anchor so the per-iteration LLM doesn't hallucinate elaborate theories from 10 samples.

### 2.2 Feature Space Explosion (p >> n) ✅ **COMPLETED**

**Current state:** 74 training samples, but feature matrices reach 30-41 columns after one-hot encoding categoricals. In run 1, iteration 5: **0/41** features survived L1 regularization (C=0.01). The model literally gave up.

**Root causes:**
- Categoricals with 5-9 values explode into 4-8 one-hot columns each
- Features only grow (swaps maintain count, but categoricals add implicit columns)
- No explicit budget on total effective dimensionality

**Proposed fixes:**
- **Hard cap on effective feature dimensionality** (post one-hot), not just raw feature count. E.g., if you have 74 train samples, cap at ~15-20 effective columns.
- **Strongly prefer binary features** over scales or categoricals (user's hunch — see 2.3)
- **Allow the LLM to specify fewer features** — don't force 1:1 swaps. Sometimes removing a bad feature without replacement is better.
- Track effective dimensionality in the prompt to the analysis LLM so it knows the budget.

### 2.3 Likert Scale Overuse ✅ **COMPLETED**

**Current state:** Almost every feature is a 1-5 scale. Issues:
- Assumes linearity in logistic regression (effect of 1→2 = effect of 4→5)
- LLM extraction is noisy on ordinal scales — the difference between a 3 and a 4 is subjective and inconsistent
- Many features are conceptually binary ("is there a privacy violation?" vs "rate the privacy violation direction 1-5")
- For continuous features where a scale makes sense, the 1-5 range is extremely coarse

**Proposed feature type guidance:**
- **Binary (bool):** Preferred for presence/absence signals. "Is there a clear prior agreement?" "Did OP offer a compromise?" "Are children directly involved?" These are unambiguous to extract and work well in logistic regression.
- **Scale (1-5 or 1-3):** Only when there's a genuine continuous dimension that matters (e.g., `harm_caused` genuinely ranges from trivial to severe). Consider 1-3 scales instead of 1-5 to reduce extraction noise.
- **Categorical:** Almost never. Only for a small number (2-3) of mutually exclusive types where the distinction genuinely changes the verdict direction differently. Each category costs one degree of freedom.

**Implementation:** Update the analysis LLM prompt to explicitly prefer binary features, explain WHY (sample size constraints, extraction noise, linearity assumption), and require justification for any non-binary feature.

### 2.4 Fixed Feature Count via Swaps ✅ **COMPLETED**

**Current state:** `max_swaps` controls how many features change per iteration. Features are always swapped 1:1 — remove N, add N. The total feature count monotonically grows (since categoricals add hidden columns) and can never shrink.

**Proposed changes:**
- Allow the LLM to propose **net removals** (remove 3, add 1 = net reduction of 2)
- Allow the LLM to propose **net additions** IF within the dimensionality budget
- Change the schema from "features_to_remove + proposed_features (same count)" to "proposed_feature_set_changes" with explicit add/remove/keep decisions
- Add a "dimensionality budget" field showing remaining capacity

### 2.5 No Formula Specification

**Current state:** The model always uses `y ~ x1 + x2 + ... + xn` — pure main effects. No interactions, no transformations.

**Proposed change:** Let the LLM specify an R-style formula that can include:
- Main effects: `y ~ harm_caused + provocation_received + ...`
- Interactions: `harm_caused:provocation_received` (e.g., harm matters more when unprovoked)
- Polynomial terms: `I(harm_caused^2)` (diminishing/accelerating effects)

**Implementation approach:**
- Use `patsy` or `formulaic` to parse R-style formulas in Python
- The LLM proposes a formula each iteration alongside feature changes
- The pipeline builds the design matrix from the formula
- Start simple (main effects only in iteration 1) and let the LLM add interactions as it learns which features interact

**Caution:** Interactions multiply the effective dimensionality. The dimensionality budget (2.2) must account for interaction terms. Start with at most 2-3 interactions.

---

## Part 3: Improving Self-Assessment

### 3.1 Larger, Smarter Validation Design

**Current state:** 74 train samples, 99-124 validation samples. Train CV is noisy (std 0.06-0.14). The validation set is fixed, which is good, but with only ~20 YTA samples, per-class metrics are extremely noisy.

**Proposed changes:**
- **Increase validation size** to 200+ if the dataset allows. The current dataset has ~5000 samples, so there's room.
- **Increase train sample size** to 100-150 per fold. The cost per sample is very low (~$0.0001 with gpt-5-mini).
- **Report confidence intervals** on validation accuracy using bootstrap or Wilson intervals. Don't trust a 2pp improvement when SE is 4pp.
- **Use macro F1 as primary metric** instead of accuracy, since the class balance is ~80/20 NTA/YTA. A model predicting all-NTA gets 80% accuracy.
  - **Empirical reminder:** In run `20260205_141120`, held-out test accuracy was ~0.57 despite ~80/20 class balance — accuracy alone is not a sensible optimization target under class imbalance + balanced class weighting.

### 3.2 Statistical Significance Gate for Feature Changes

**Current state:** Features are swapped based on coefficient magnitude and LLM reasoning. No statistical test checks whether the change actually helped.

**Proposed change:** After each iteration, compare the new feature set's validation performance against the previous best using a simple significance test (e.g., McNemar's test on paired predictions, or bootstrap confidence interval on accuracy difference). Only accept changes that show at least a non-negative trend. If the change hurts, revert.

### 3.3 Ablation-Aware Feature Evaluation

**Current state:** Feature importance is judged by L1 coefficient magnitude. But with correlated features, L1 arbitrarily picks one and zeros the other — this doesn't mean the zeroed feature is useless.

**Proposed change:** For features that L1 zeroes out, run a quick ablation: drop the feature, refit, see if validation accuracy changes. If it doesn't change, the feature is truly redundant. If it drops, the feature provides information that happens to be captured by a correlated feature — in which case, the right move might be to keep it and drop the correlated one instead.

---

## Part 4: Improving Self-Improvement

### 4.1 Broad Pattern Discovery (New Pre-Loop Step)

Before the iteration loop starts, run a "discovery" step:

```
Input: 200 random posts (text + verdict only, no features)
Prompt: "What patterns distinguish NTA from YTA posts? 
         Focus on narrative structure, framing, social norms, 
         relationship dynamics. Be concrete and specific."
Output: 10-15 pattern descriptions
```

This gives the pipeline a map of the terrain before it starts navigating. The per-iteration analysis LLM references these patterns and asks "which of these known patterns are not yet captured by features?" rather than rediscovering patterns from scratch each iteration.

**Cost:** ~$0.01 for 200 posts with gpt-5. One-time per session.

### 4.2 Feature Proposal Grounding

**Current state:** The analysis LLM proposes features in a vacuum based on 10 samples.

**Proposed change:** The feature proposal prompt should include:
1. The broad patterns (from 4.1)
2. Current effective dimensionality and remaining budget
3. Explicit preference hierarchy: binary > 1-3 scale > 1-5 scale >>> categorical
4. A "coverage audit": for each broad pattern, which feature (if any) captures it?
5. Previous features that were tried and dropped (already implemented via `tried_features`)

This grounds proposals in real domain patterns rather than sample-specific hallucinations.

### 4.3 Feature Quality Validation Before Committing

**Current state:** Proposed features are immediately added and used.

**Proposed change:** Before committing a new feature to the main loop, run a quick sanity check:
1. Extract the proposed feature on a small held-out sample (20-30 posts)
2. Check: Does it have variance? (A feature that's always 3/5 is useless)
3. Check: Does it correlate with the label at all? (point-biserial for binary, Spearman for scale)
4. Check: Is it redundant with an existing feature? (correlation > 0.8 with any current feature)

Only features passing these checks get added. This prevents wasting an iteration on features that are poorly extracted or redundant.

### 4.4 Separate "Extraction Quality" from "Predictive Value"

**Current state:** If a feature has a near-zero coefficient, the LLM assumes it's a bad feature idea. But it might be a great idea that's being poorly extracted.

**Proposed change:** When evaluating features, show the LLM:
1. The coefficient (current)
2. The feature's value distribution (is it always the same value? That's an extraction problem)
3. The feature's bivariate correlation with the label (does it correlate in isolation?)
4. A few extraction examples (text → extracted value) so the LLM can judge extraction quality

This lets the LLM distinguish "bad concept" from "bad extraction" and refine descriptions rather than discarding good ideas.

---

## Part 5: Implementation Priority

### Phase 1: Bug Fixes (Must Do Before Any More Runs) ✅ **COMPLETED**
1. ✅ **Fix feature cache** to include feature config hash (1.1)
2. ✅ **Fix per-class metrics** to use actual train→validate predictions (1.2)
3. ✅ **Add column alignment assertion** between train and validation (1.3)

### Phase 2: Core Loop Improvements (High Impact)
4. ✅ **Add broad pattern discovery** pre-loop step (4.1)
5. ✅ **Shift to binary-preferred features** with explicit guidance in prompts (2.3)
6. ✅ **Allow variable feature count** with dimensionality budget (2.2, 2.4)
7. **Add feature quality validation** before committing (4.3)
8. **Switch primary metric to macro F1** with confidence intervals (3.1)

### Phase 3: Advanced Modeling (Medium Impact)
9. **Add R-style formula specification** (2.5)
10. **Add statistical significance gate** for accepting changes (3.2)
11. **Add ablation-aware feature evaluation** (3.3)
12. **Show extraction quality diagnostics** to the analysis LLM (4.4)

### Phase 4: Efficiency & Polish
13. **Increase sample sizes** (train to 100-150, validation to 200+) (3.1)
14. **Ground feature proposals** with coverage audit (4.2)
15. Clean up verbose logging to track new metrics

---

## Appendix: Evidence from Run Analysis

### Run Summaries

| Run | Mode | Location | Iterations | Best Val Acc | Notes |
|-----|------|----------|------------|-------------|-------|
| 20260205_104115 | legacy | iteration_logs | 3 | N/A (CV only) | Early test, no val split |
| 20260205_105556 | legacy | iteration_logs | 3 | N/A (CV only) | Early test, no val split |
| 20260205_112254 | legacy | iteration_logs | 6 | N/A (CV only) | Longest legacy run |
| 20260205_122742 | val-split | demo_run | 4 | 0.831 | Per-class metrics frozen (cache bug) |
| 20260205_125425 | val-split | demo_run | 3 | 0.838 | Per-class metrics frozen (cache bug) |

### Key Failure Patterns

1. **Validation collapse:** Both val-split runs show iterations where validation accuracy drops to ~0.17 (worse than random). This happens when many categoricals are added and the feature matrix becomes too wide for the sample size. The cache bug makes recovery impossible since the pipeline can't measure the damage.

2. **Feature accumulation:** Features grow from 8 → 17+ across iterations. With one-hot encoding, effective dimensionality reaches 30-41. At 74 training samples, this is severe overfitting territory.

3. **Repetitive LLM insights:** The sample analysis LLM independently rediscovers the same patterns every iteration ("crowd rewards chronicity and receipts", "crowd punishes controlling behavior") because it has no memory of the broad landscape. This wastes token budget and leads to redundant feature proposals.

4. **Categorical explosion:** `canonical_trope_cluster` (9 values = 8 one-hot columns), `agreement_presence_and_violation_by_party` (6 values = 5 columns), `evidence_or_corroboration_level` (4 values = 3 columns). These three features alone consume 16 degrees of freedom from a 74-sample budget.

5. **Scale features with no variance:** Many 1-5 scale features cluster around 3 with low variance, providing minimal signal. Binary features would be more reliable to extract and more statistically efficient.
